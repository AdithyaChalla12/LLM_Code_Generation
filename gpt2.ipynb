{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vrRfUV2W1hM"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# LIGHTWEIGHT GPT MODEL FINE-TUNING FOR API GENERATION\n",
        "# ============================\n",
        "\n",
        "!pip install -q transformers datasets accelerate evaluate\n",
        "# Add this RIGHT AFTER the pip install, BEFORE any other imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "\n",
        "# ============================\n",
        "# MODEL SELECTION - CHOOSE YOUR LIGHTWEIGHT MODEL\n",
        "# ============================\n",
        "MODEL_OPTIONS = {\n",
        "    \"distilgpt2\": {\n",
        "        \"name\": \"distilgpt2\",  # 82M parameters (smallest, fastest)\n",
        "        \"context_length\": 512,\n",
        "        \"description\": \"Smallest & fastest, good for quick experiments\"\n",
        "    },\n",
        "    \"gpt2\": {\n",
        "        \"name\": \"gpt2\",  # 124M parameters\n",
        "        \"context_length\": 1024,\n",
        "        \"description\": \"Small GPT-2, balanced speed and performance\"\n",
        "    },\n",
        "    \"gpt2-medium\": {\n",
        "        \"name\": \"gpt2-medium\",  # 355M parameters\n",
        "        \"context_length\": 1024,\n",
        "        \"description\": \"Medium GPT-2, better quality but slower\"\n",
        "    },\n",
        "    \"codeparrot-small\": {\n",
        "        \"name\": \"codeparrot/codeparrot-small\",  # 110M parameters\n",
        "        \"context_length\": 512,\n",
        "        \"description\": \"Specialized for code, very lightweight\"\n",
        "    },\n",
        "    \"microsoft-codebert\": {\n",
        "        \"name\": \"microsoft/codebert-base\",  # 125M parameters\n",
        "        \"context_length\": 512,\n",
        "        \"description\": \"Trained on code, good for API tasks\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# SELECT YOUR MODEL HERE\n",
        "SELECTED_MODEL = \"distilgpt2\"  # Change this to select different model\n",
        "MODEL_CONFIG = MODEL_OPTIONS[SELECTED_MODEL]\n",
        "MODEL_NAME = MODEL_CONFIG[\"name\"]\n",
        "MAX_LENGTH = MODEL_CONFIG[\"context_length\"]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"ü§ñ Selected Model: {MODEL_NAME}\")\n",
        "print(f\"üìù Description: {MODEL_CONFIG['description']}\")\n",
        "print(f\"üìè Max Context Length: {MAX_LENGTH}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============================\n",
        "# CONFIGURATION\n",
        "# ============================\n",
        "# Paths\n",
        "DATASET_PATH = \"/content/drive/MyDrive/API-Pack-ALL-CLEANED\"\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/{SELECTED_MODEL}-finetuned-api\"\n",
        "CHECKPOINT_DIR = f\"/content/drive/MyDrive/{SELECTED_MODEL}-checkpoints\"\n",
        "BEST_MODEL_DIR = f\"/content/drive/MyDrive/{SELECTED_MODEL}-best-model\"\n",
        "\n",
        "# Training Configuration\n",
        "SPEED_MODE = \"fast\"  # Options: \"test\", \"fast\", \"full\"\n",
        "\n",
        "configs = {\n",
        "    \"test\": {\n",
        "        \"train_fraction\": 0.01,    # 1% data for testing\n",
        "        \"val_fraction\": 0.01,\n",
        "        \"num_epochs\": 1,\n",
        "        \"batch_size\": 8,\n",
        "        \"eval_steps\": 500,\n",
        "        \"save_steps\": 500,\n",
        "        \"learning_rate\": 5e-5,\n",
        "    },\n",
        "    \"fast\": {\n",
        "        \"train_fraction\": 0.1,     # 10% data for quick training\n",
        "        \"val_fraction\": 0.05,\n",
        "        \"num_epochs\": 3,\n",
        "        \"batch_size\": 4,            # Smaller batch for GPT models\n",
        "        \"eval_steps\": 200,\n",
        "        \"save_steps\": 200,\n",
        "        \"learning_rate\": 3e-5,\n",
        "    },\n",
        "    \"full\": {\n",
        "        \"train_fraction\": 1.0,      # Full dataset\n",
        "        \"val_fraction\": 0.1,\n",
        "        \"num_epochs\": 5,\n",
        "        \"batch_size\": 4,\n",
        "        \"eval_steps\": 100,\n",
        "        \"save_steps\": 100,\n",
        "        \"learning_rate\": 2e-5,\n",
        "    }\n",
        "}\n",
        "\n",
        "config = configs[SPEED_MODE]\n",
        "print(f\"\\n‚öôÔ∏è  Training Mode: {SPEED_MODE.upper()}\")\n",
        "print(f\"   Data: {config['train_fraction']*100:.0f}% training, {config['val_fraction']*100:.0f}% validation\")\n",
        "print(f\"   Epochs: {config['num_epochs']}\")\n",
        "\n",
        "# Create directories\n",
        "for dir_path in [OUTPUT_DIR, CHECKPOINT_DIR, BEST_MODEL_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üñ•Ô∏è  Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# ============================\n",
        "# LOAD AND PREPARE DATASET\n",
        "# ============================\n",
        "print(\"\\nüì¶ Loading dataset...\")\n",
        "dataset = load_from_disk(DATASET_PATH)\n",
        "\n",
        "# Sample dataset based on configuration\n",
        "original_train_size = len(dataset[\"train\"])\n",
        "original_val_size = len(dataset[\"validation\"])\n",
        "\n",
        "train_size = int(original_train_size * config[\"train_fraction\"])\n",
        "val_size = int(original_val_size * config[\"val_fraction\"])\n",
        "\n",
        "dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
        "dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(val_size))\n",
        "\n",
        "print(f\"üìä Dataset sizes:\")\n",
        "print(f\"   Training: {len(dataset['train']):,} samples\")\n",
        "print(f\"   Validation: {len(dataset['validation']):,} samples\")\n",
        "\n",
        "# ============================\n",
        "# LOAD TOKENIZER AND MODEL\n",
        "# ============================\n",
        "print(f\"\\nüî§ Loading tokenizer for {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# GPT models need padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"ü§ñ Loading model {MODEL_NAME}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.to(device)\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable()\n",
        "    print(\"‚úÖ Gradient checkpointing enabled\")\n",
        "\n",
        "# Print model size\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"üìè Model size: {total_params/1e6:.1f}M parameters ({trainable_params/1e6:.1f}M trainable)\")\n",
        "\n",
        "# ============================\n",
        "# DATA PREPROCESSING FOR GPT\n",
        "# ============================\n",
        "def preprocess_for_gpt(examples):\n",
        "    \"\"\"\n",
        "    Format data for GPT-style training.\n",
        "    We'll use a prompt template for API generation.\n",
        "    \"\"\"\n",
        "    # Create prompt-completion pairs\n",
        "    prompts = []\n",
        "    for source, target in zip(examples[\"source\"], examples[\"target\"]):\n",
        "        # Format: \"### Instruction: {source}\\n### Response: {target}\"\n",
        "        prompt = f\"### Instruction: {source}\\n### Response: {target}{tokenizer.eos_token}\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Tokenize\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # For language modeling, labels are the same as input_ids\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
        "\n",
        "    # Replace padding token id's in labels by -100 so they're ignored by loss\n",
        "    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "print(\"\\nüîÑ Tokenizing datasets...\")\n",
        "tokenized_datasets = dataset.map(\n",
        "    preprocess_for_gpt,\n",
        "    batched=True,\n",
        "    num_proc=2,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# CUSTOM METRICS\n",
        "# ============================\n",
        "import evaluate\n",
        "import re\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"Compute metrics for generation quality\"\"\"\n",
        "    predictions, labels = eval_preds\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Extract generated responses (after \"### Response:\")\n",
        "    generated_responses = []\n",
        "    for pred in decoded_preds:\n",
        "        if \"### Response:\" in pred:\n",
        "            response = pred.split(\"### Response:\")[-1].strip()\n",
        "            generated_responses.append(response)\n",
        "        else:\n",
        "            generated_responses.append(pred)\n",
        "\n",
        "    # Extract expected responses\n",
        "    expected_responses = []\n",
        "    for label in decoded_labels:\n",
        "        if \"### Response:\" in label:\n",
        "            response = label.split(\"### Response:\")[-1].strip()\n",
        "            expected_responses.append(response)\n",
        "        else:\n",
        "            expected_responses.append(label)\n",
        "\n",
        "    # Calculate metrics\n",
        "    exact_matches = sum(1 for gen, exp in zip(generated_responses, expected_responses)\n",
        "                       if gen.strip() == exp.strip())\n",
        "\n",
        "    # Simple BLEU calculation\n",
        "    try:\n",
        "        bleu = evaluate.load(\"sacrebleu\")\n",
        "        bleu_result = bleu.compute(\n",
        "            predictions=generated_responses[:10],  # Sample for speed\n",
        "            references=[[exp] for exp in expected_responses[:10]]\n",
        "        )\n",
        "        bleu_score = bleu_result['score'] / 100\n",
        "    except:\n",
        "        bleu_score = 0.0\n",
        "\n",
        "    return {\n",
        "        \"exact_match\": exact_matches / len(generated_responses),\n",
        "        \"bleu\": bleu_score\n",
        "    }\n",
        "\n",
        "# ============================\n",
        "# CHECKPOINT MANAGEMENT\n",
        "# ============================\n",
        "checkpoint_info_path = os.path.join(CHECKPOINT_DIR, \"checkpoint_info.json\")\n",
        "\n",
        "class GPTTrainerWithCheckpoints(Trainer):\n",
        "    \"\"\"Custom trainer with checkpoint management\"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.best_metric = float('-inf')\n",
        "        self.checkpoint_info = {\n",
        "            \"best_metric\": self.best_metric,\n",
        "            \"best_checkpoint\": None,\n",
        "            \"training_history\": []\n",
        "        }\n",
        "\n",
        "        # Load existing checkpoint info\n",
        "        if os.path.exists(checkpoint_info_path):\n",
        "            with open(checkpoint_info_path, 'r') as f:\n",
        "                self.checkpoint_info = json.load(f)\n",
        "                self.best_metric = self.checkpoint_info.get(\"best_metric\", float('-inf'))\n",
        "\n",
        "    def _save_checkpoint(self, model, trial, metrics=None):\n",
        "        super()._save_checkpoint(model, trial, metrics)\n",
        "\n",
        "        if metrics and \"eval_bleu\" in metrics:\n",
        "            self.checkpoint_info[\"training_history\"].append({\n",
        "                \"step\": self.state.global_step,\n",
        "                \"metrics\": metrics,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            })\n",
        "\n",
        "            if metrics[\"eval_bleu\"] > self.best_metric:\n",
        "                self.best_metric = metrics[\"eval_bleu\"]\n",
        "                self.checkpoint_info[\"best_metric\"] = self.best_metric\n",
        "                self.checkpoint_info[\"best_checkpoint\"] = self.state.global_step\n",
        "\n",
        "                print(f\"\\nüèÜ New best model! BLEU: {self.best_metric:.4f}\")\n",
        "                self.model.save_pretrained(BEST_MODEL_DIR)\n",
        "                self.tokenizer.save_pretrained(BEST_MODEL_DIR)\n",
        "\n",
        "            # Save checkpoint info\n",
        "            with open(checkpoint_info_path, 'w') as f:\n",
        "                json.dump(self.checkpoint_info, f, indent=2)\n",
        "\n",
        "# ============================\n",
        "# TRAINING SETUP\n",
        "# ============================\n",
        "print(\"\\n‚öôÔ∏è  Setting up training...\")\n",
        "\n",
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # GPT uses causal LM, not masked LM\n",
        "    pad_to_multiple_of=8  # Efficient padding\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=config[\"num_epochs\"],\n",
        "    per_device_train_batch_size=config[\"batch_size\"],\n",
        "    per_device_eval_batch_size=config[\"batch_size\"] * 2,\n",
        "    eval_steps=config[\"eval_steps\"],\n",
        "    save_steps=config[\"save_steps\"],\n",
        "    warmup_steps=100,\n",
        "    learning_rate=config[\"learning_rate\"],\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision on GPU\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = GPTTrainerWithCheckpoints(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,\n",
        "            early_stopping_threshold=0.001\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# TRAIN\n",
        "# ============================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèãÔ∏è  STARTING GPT FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Training samples: {len(tokenized_datasets['train']):,}\")\n",
        "print(f\"Batch size: {config['batch_size']} (effective: {config['batch_size']*2})\")\n",
        "print(f\"Epochs: {config['num_epochs']}\")\n",
        "print(f\"Learning rate: {config['learning_rate']}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Train\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    # Save final model\n",
        "    print(\"\\nüíæ Saving final model...\")\n",
        "    trainer.save_model(OUTPUT_DIR)\n",
        "    trainer.save_state()\n",
        "\n",
        "    training_time = (time.time() - start_time) / 60\n",
        "    print(f\"\\n‚úÖ Training completed in {training_time:.1f} minutes!\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\nüìä Final evaluation...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    print(\"\\nüìà Final Metrics:\")\n",
        "    for key, value in eval_results.items():\n",
        "        if key.startswith(\"eval_\"):\n",
        "            metric_name = key.replace(\"eval_\", \"\")\n",
        "            print(f\"   {metric_name}: {value:.4f}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è  Training interrupted!\")\n",
        "    trainer.save_model(os.path.join(CHECKPOINT_DIR, \"interrupted\"))\n",
        "    print(\"Checkpoint saved.\")\n",
        "\n",
        "# ============================\n",
        "# TEST THE FINE-TUNED MODEL\n",
        "# ============================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üß™ TESTING FINE-TUNED GPT MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load best model\n",
        "if os.path.exists(BEST_MODEL_DIR):\n",
        "    print(f\"Loading best model from {BEST_MODEL_DIR}\")\n",
        "    test_model = AutoModelForCausalLM.from_pretrained(BEST_MODEL_DIR)\n",
        "    test_tokenizer = AutoTokenizer.from_pretrained(BEST_MODEL_DIR)\n",
        "else:\n",
        "    print(f\"Loading final model from {OUTPUT_DIR}\")\n",
        "    test_model = model\n",
        "    test_tokenizer = tokenizer\n",
        "\n",
        "test_model = test_model.to(device)\n",
        "test_model.eval()\n",
        "\n",
        "# Test generation function\n",
        "def generate_api_code(instruction, max_length=256):\n",
        "    \"\"\"Generate API code from instruction\"\"\"\n",
        "    prompt = f\"### Instruction: {instruction}\\n### Response:\"\n",
        "\n",
        "    inputs = test_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = test_model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=test_tokenizer.eos_token_id,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "\n",
        "    generated_text = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the response part\n",
        "    if \"### Response:\" in generated_text:\n",
        "        response = generated_text.split(\"### Response:\")[-1].strip()\n",
        "        return response\n",
        "    return generated_text\n",
        "\n",
        "# Test examples\n",
        "test_examples = [\n",
        "    \"Create a Python function to fetch user data from GitHub API\",\n",
        "    \"Write code to post a message to Slack webhook\",\n",
        "    \"Generate a request to OpenWeatherMap API for current weather\"\n",
        "]\n",
        "\n",
        "print(\"\\nüìù Testing with examples:\\n\")\n",
        "for i, example in enumerate(test_examples, 1):\n",
        "    print(f\"Example {i}:\")\n",
        "    print(f\"Instruction: {example}\")\n",
        "    print(f\"Generated: {generate_api_code(example)[:200]}...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n‚úÖ Fine-tuning complete!\")\n",
        "print(f\"üìÅ Models saved:\")\n",
        "print(f\"   - Best model: {BEST_MODEL_DIR}\")\n",
        "print(f\"   - Final model: {OUTPUT_DIR}\")\n",
        "\n",
        "# ============================\n",
        "# INTERACTIVE TESTING\n",
        "# ============================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéÆ INTERACTIVE API CODE GENERATION\")\n",
        "print(\"Type 'quit' to exit\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nüìù Enter API instruction (or 'quit'): \").strip()\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    print(\"\\n‚öôÔ∏è  Generating...\")\n",
        "    generated = generate_api_code(user_input)\n",
        "    print(f\"\\nüéØ Generated Code:\\n{generated}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hX-uqF1kYppR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}